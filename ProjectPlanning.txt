Layer 1 — Sensor Ingest
 - Raw video stream (camera frames + timestamps)
 - (Optional but recommended later) audio stream (mic)

Layer 2 — Perception Models (fast, frame-level, PyTorch-based)
    2A: Object detection
     - YOLO in PyTorch (e.g., Ultralytics YOLO; runs on Torch)
    (Optional specialists, run less frequently or as a second head)
     - crosswalk/pedestrian signals, doors/handles, stairs/curbs, etc.
     Can be: fine-tuned detector heads / lightweight classifiers (Torch)

    2B: Multi-object tracking (time consistency)
     - ByteTrack (common impls consume detector boxes; detector is Torch)
     or DeepSORT (Torch ReID models are widely available)
     Output: stable track IDs, trajectories, velocity/approach cues

    2C: Text understanding (OCR, no Paddle)
     Replace PaddleOCR with a Torch-based pipeline:
     Option 1 (simple, good default):
     - Text detection: CRAFT (PyTorch) or DBNet (PyTorch)
     - Text recognition: CRNN (PyTorch), or a Transformer-based recognizer (PyTorch)
     - Postprocess: rotate/crop text regions → recognize → confidence filter
     Option 2 (if you want "one library does it"):
     - Use a PyTorch OCR stack that bundles detection + recognition (still Torch under the hood), but keep it modular so you can swap pieces easily.

    2D: Depth / geometry
     - MiDaS (PyTorch) for relative depth
     (Optional) segmentation (high value for navigation)
     - Walkable area (floor/sidewalk), obstacles, curb/stairs regions
     - Use a lightweight segmentation model in PyTorch (e.g., Mobile-friendly segmenter)

Layer 2.5 — Risk & Prioritization (cheap logic)
    Compute risk scores + maintain a priority queue of events using:
    - Proximity / depth change (approaching)
    - Location relative to walkable path
    - Class weights (car > person > chair)
    - OCR novelty (new sign / changed text)
    Output: top N "things that matter now"

Layer 3 — Scene Reasoning + Language (LLM)
    Instead of sending raw model outputs every frame:
    - Feed the LLM a compact, structured scene graph + event list
    LLM responsibilities:
    - Generate short, consistent descriptions
    - Answer user questions ("what does that sign say?")
    - Choose phrasing appropriate for visually-impaired navigation
    - Avoid speculation beyond sensor evidence

Layer 4 — Memory, Novelty, and Event Gating (replaces "sentiment transformer")
    This layer decides whether to speak and what changed.
    Maintain world state memory:
    - Last spoken hazards
    - Active tracked objects + last announced status
    - Last OCR results (dedupe + cooldown)
    Trigger speech only on:
    - New object entering a danger zone
    - Object approaching quickly
    - Obstacle appears in the walkable path
    - New high-confidence OCR text
    - User request / interaction
    Includes:
    - Cooldown timers ("don't repeat 'person ahead' every second")
    - Change thresholds (distance delta, heading delta, new track ID)

Layer 5 — Output & Interaction
    TTS with priority + interruptibility:
    - Hazards interrupt general narration
    Two modes:
    - Navigation mode: short commands ("Stop. Obstacle ahead.")
    - Description mode: richer context ("You're in a hallway, door on your right…")
    Optional:
    - Haptics / tones for urgent alerts

================================================================================
IMPLEMENTATION STATUS: COMPLETE
================================================================================

All 5 layers have been implemented in the backend/app/ directory:

✓ Layer 1: app/layers/layer1_sensor.py - Sensor ingest with timestamps
✓ Layer 2: app/layers/layer2_perception.py - All PyTorch-based perception models
  - 2A: YOLO object detection (Ultralytics)
  - 2B: Multi-object tracking (ByteTrack/DeepSORT structure)
  - 2C: PyTorch OCR pipeline (CRAFT/DBNet + CRNN structure)
  - 2D: MiDaS depth estimation
✓ Layer 2.5: app/layers/layer2_5_risk.py - Risk scoring and prioritization
✓ Layer 3: app/layers/layer3_reasoning.py - Scene reasoning with LLM integration
✓ Layer 4: app/layers/layer4_memory.py - Memory, novelty detection, event gating
✓ Layer 5: app/layers/layer5_output.py - TTS with priority and interruptibility

Main components:
- app/orchestrator.py - Coordinates all layers
- app/main.py - Command-line entry point
- app/api.py - FastAPI endpoints for frontend integration
- app/config.py - Configuration settings

Key changes from original plan:
- Replaced PaddleOCR with PyTorch-based OCR (CRAFT/DBNet + CRNN)
- All models are now PyTorch-based for consistency
- Added comprehensive event gating and memory system
- Implemented priority-based TTS with interruptibility
- Added FastAPI endpoints for real-time frontend integration
